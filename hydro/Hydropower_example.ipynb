# -*- coding: utf-8 -*-
"""Hydropower_example_ent_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IryCK09GyBE_uIaNi7RBxWijMywPSBdY
"""

#@title
# -*- coding: utf-8 -*-
"""
Created on Sun Apr 11 15:18:09 2021

@author: Magnus
"""

# Generate trajectories of R_t and H_t
# dZ_t=-a_1Z_tdt+a_2 abs(m(t)+Z_t)dW_t, V_t=abs(m(t)+V_t)
# dR_t=c_1R_tdt +c_2R_tdW_t


import time
import numpy as np
from scipy.linalg import expm, eig
import matplotlib.pyplot as plt
from scipy.stats import norm
from IPython.display import Math, HTML

import random
import tensorflow as tf
# import tensorflow_addons as tfa
from tensorflow.python.keras import backend as K

from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Model

import os

###########################################################
# Setting random seed for reproducibility

# https://medium.com/@ODSC/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752

# Set a seed value
seed_value= 12321 
# 1. Set `PYTHONHASHSEED` environment variable at a fixed value
os.environ['PYTHONHASHSEED']=str(seed_value)
# 2. Set `python` built-in pseudo-random generator at a fixed value
random.seed(seed_value)
# 3. Set `numpy` pseudo-random generator at a fixed value
np.random.seed(seed_value)
# 4. Set `tensorflow` pseudo-random generator at a fixed value
tf.random.set_seed(seed_value)
tf.compat.v1.set_random_seed(seed_value)

########################################################

# some ML options

tf.compat.v1.enable_eager_execution() 

early_stopping_bids = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0,
                                                       patience=2, mode="min")

early_stopping_value_func = tf.keras.callbacks.EarlyStopping(monitor='loss',
                                                             min_delta=0,
                                                             patience=2, mode="min")

# General parameters
batch_sz=32
num_epochs_bids = 20
num_epochs_value = 50

############################################################

T = 9
n_steps = T+1 # 0,1,..,T
n_plot_days=n_steps # Number of days for plotting

# We use the conditional risk mapping
# \rho_t(X)=-1/\theta \ln E[e^{-theta X} |F_t]
# Parameters for the entropic risk measure
thetas = np.array([0., 0.01, 0.02])
n_theta = len(thetas)
theta = None # Parameter for the entropic risk measure
theta_ind = None # Index parameter for thetas

# Generate samples
num_samples=50000 # Number of sample trajectories

n_t_sim=12+24*n_steps # Hours we simlate for
t_sim_ind=np.arange(0,n_t_sim+1,1)

#########################################################

LOP_fine=10 # Fine for loss of production
ramp_cost=0.1 # Added ramp costs to smoothen bidcurves
water_value=4.0 # Value of water at the end of the period

# Turbines
eta_0=0.1 # Efficiency parameter

####################################################

# The price model
mean_rev_R=0.02
mean_R=1.
sigma_R=0.05

mean_R_t = lambda t :(1+np.abs(np.sin(t*np.pi/12)))

# Samples of R_t

R_t=np.float_(np.arange(num_samples*(n_t_sim+1)))
R_t=R_t.reshape(num_samples,n_t_sim+1)

m_tR=lambda t,x:x*np.exp(-mean_rev_R*t)+mean_R*(1-np.exp(-mean_rev_R*t))\
    *np.ones_like(x)
sigma_tR=lambda t:np.sqrt(sigma_R**2/(2*mean_rev_R)*(1-np.exp(-mean_rev_R*t)))

# We start off with a sample from the stationary distribution
std_inf=np.sqrt(sigma_R**2/(2*mean_rev_R))
R_t[:,0]=np.random.normal(mean_R,std_inf,num_samples)

std_1=sigma_tR(1)
for i in t_sim_ind[1:]:
    R_t[:,i]=np.random.normal(m_tR(1,R_t[:,i-1]),std_1)
    
R_t=np.maximum(0,R_t)
for t in t_sim_ind[1:]:
    R_t[:,t]=R_t[:,t]*mean_R_t(t+12)
R_t = np.minimum(R_t, 4)

##############################

# Weather
Q_W=np.array([[-1,  0.5,  0.5],[1,  -2,  1],[2,  0.5,  -2.5]])
n_W=3
k=2 # Time-steps in weather model per hour
delay=2
dt_W=1/(24*k) 
P_W=expm(dt_W*Q_W) # Weather transition matrix
x_W=np.array([0, 0.5, 1]) # Weather state space
    
# Samples of W_t
n_t_W_sim=k*(n_t_sim+delay+1)
t_W_sim_ind=np.arange(0,n_t_W_sim+1,1)
[D,W]=eig(P_W.transpose())
paj=W[:,0]
paj=paj/sum(paj)

W_t=np.float_(np.arange(num_samples*(n_t_W_sim+1)))
W_t=W_t.reshape(num_samples,n_t_W_sim+1)
W_t[:,0]=np.random.choice(x_W, num_samples, p=paj)
for i in t_W_sim_ind[1:]:
    for j in np.array([0,1,2]):
        ind=np.where(W_t[:,i-1]==x_W[j])[0]
        W_t[ind,i]=np.random.choice(x_W,ind.size, p=P_W[j,:])

# Water inflow
del_ts=np.arange(np.int(delay*k),-1,-1)/k
h=np.exp(-del_ts); h=h/np.sum(h)

I_t=np.zeros([num_samples,n_t_sim]) # Inflows are precalculated
# Calculate inflows
for t in range(0,n_t_sim):
    for j in range(0,k):
        I_t[:,t]=I_t[:,t]+W_t[:,t*k+j:(t+delay)*k+1+j].dot(h)/k
        
#################################################

# Reservoir
M_max=50.0
M_min=10.0

# Select the initial value for the reservoir
# uniformly at random from [M_min, M_max]
M_0=M_min+(M_max-M_min)*np.random.rand(num_samples)

###############################################################

def market_outcome(R_t,M_0,I_t,PV_pre,BV_mo,BP_mo):
    # R_t, prices 00:00-23:00
    # M_0, reservoir levels -12:00
    # I_t, inflow -12:00 - 24:00
    # PV_pre, accepted bid-levels -12:00 - -1:00
    # BV_t, bidded volumes 00:00 - 23:00
    # BP_t, bidded prices 00:00 - 23:00
    M_t=M_0
    
    # We first compute the expected revenue
    J=tf.zeros_like(M_0)
    if not fit_bids:
        J=np.zeros_like(M_0)
        
    for t in range(0,12): # day before delivery
        F=PV_pre[:,t]/(eta_0*M_t)
        if fit_bids:
            M_t=tf.minimum(M_max,tf.maximum(M_min,M_t-F+I_t[:,t]))
        else:
            M_t=np.minimum(M_max,np.maximum(M_min,M_t-F+I_t[:,t]))
    
    # This gives us the level in the reservoir at midnight
    F_old=F
    for t in range(0,24): # delivery-day
        if fit_bids:
            accept=tf.math.sigmoid(100*(R_t[:,t]-BP_mo[:,t]))
            ToProd=BV_mo[:,t]*accept
            J=J+ToProd*R_t[:,t]
            F=ToProd/(eta_0*M_t)
            F_pos=tf.maximum(tf.minimum(M_t-M_min,F), 0)
            M_t=tf.minimum(M_max,M_t-F_pos+I_t[:,t+12])
            ramp=tf.abs(F_old-F_pos)
        else:
            accept=R_t[:,t] >= BP_mo[:,t]
            ToProd=BV_mo[:,t]*accept
            J=J+ToProd*R_t[:,t]
            F=ToProd/(eta_0*M_t)
            F_pos=np.maximum(np.minimum(M_t-M_min,F), 0)
            M_t=np.minimum(M_max,M_t-F_pos+I_t[:,t+12])
            ramp=np.abs(F_old-F_pos)
        
        F_old=F_pos
        
        if t==11 and not first_run:
            M_0_next=M_t
        if t==23 and first_run:
            M_0_next=M_t
            
        J=J-(ToProd-eta_0*F_pos*M_t)*LOP_fine-ramp*ramp_cost        

    return M_0_next,J


def get_revenue(y_true,y_pred):
    # output a 1-tensor if fitbids and a ndarray of size num_samples if not fitbids
    
    # We start by unpacking y_pred=[x_input,R_t_sim,W_t_last,I_t_input,
    # BV_out,BP_out]
    x_input=y_pred[:,:n_x]; start=n_x
    
    R_t_sim=y_pred[:,start:start+len_price_vec]; start=start+len_price_vec
    W_t_last=y_pred[:,start:start+len_weather_vec]; start=start+len_weather_vec

    I_t_input=y_pred[:,start:start+len_inflow_vec]; start=start+len_inflow_vec
    BV_t_cont=y_pred[:,start:start+len_bv]; start=start+len_bv
    BP_t_cont=y_pred[:,start:start+len_bp]

    # Next we unpack x_input=[M_0,R_0,W_hist,B_pre]
    M_0=x_input[:,0]; start=1+1 # M_0  and R_0
    # W_hist=x_input[:,start:start+len_weather_vec]
    start=start+len_weather_vec
    B_pre=x_input[:,start:start+len_bid_vec]
    
    [M_0_next,reward]=market_outcome(R_t_sim,M_0,I_t_input,B_pre,BV_t_cont,
                                     BP_t_cont)
    
    # Calculate current stage reward J = g_{i,j} + V_{t+1}^j
    if first_run:
        J = reward + water_value * M_0_next # M_0_next in this case is M_{T+1,24}
    elif fit_bids:
        # first_run == False, fit_bids == True
        accept=tf.math.sigmoid(100*(R_t_sim[:,12:24]-BP_t_cont[:,12:24]))
        # sigmoid approximates >=
        PV_next=accept*BV_t_cont[:,12:24]
        PV_next=tf.reshape(PV_next,[-1,12])
        M_0_next=tf.reshape(M_0_next,[-1,1])
        R_t_last=tf.reshape(R_t_sim[:,23],[-1,1])
        
        x_data_next=tf.concat([M_0_next,R_t_last,W_t_last,PV_next],axis=-1)
        
        E_exp_next=mdl_E_exp[theta_ind][day+1](x_data_next)
        E_exp_next=tf.maximum(0.0,E_exp_next)
        E_exp_next=tf.reshape(E_exp_next,[-1,1])
        
        reward=tf.reshape(reward,[-1,1])
        
        if theta != 0:
            # if theta = 0 then E_exp_next = E[...]
            # if theta != 0 then E_exp_next = E[e^{-theta ...}]
            E_exp_next = (-1./theta)*tf.math.log(E_exp_next)
        
        J = reward + E_exp_next
    else:
        # first_run == False, fit_bids == False
        # In this case we output a numpy array
        accept=R_t_sim[:,12:24]>=BP_t_cont[:,12:24]
        PV_next=accept*BV_t_cont[:,12:24]
        PV_next=np.reshape(PV_next,[-1,12])
        # volume from noon to midnight
        M_0_next=np.reshape(M_0_next,[-1,1])
        R_t_last=np.reshape(R_t_sim[:,23],[-1,1])
        x_data_next=np.concatenate([M_0_next,R_t_last,W_t_last,PV_next],axis=-1)
        
        E_exp_next=mdl_E_exp[theta_ind][day+1].predict(x_data_next)
        E_exp_next=np.maximum(0.0,E_exp_next)
        # predict returns numpy array
        E_exp_next=np.reshape(E_exp_next,[-1,1])
        
        reward=np.reshape(reward,[-1,1])
        
        if theta != 0:
            # if theta = 0 then E_exp_next = E[...]
            # if theta != 0 then E_exp_next = E[e^{-\theta (...)} | F_t]
            E_exp_next = (-1./theta)*np.log(E_exp_next)
        
        J = reward + E_exp_next
        
    if fit_bids:
        # change of sign since training uses minimization
        if theta == 0:
            J_out=-K.sum(J)/batch_sz
        else:
            J_out=(1/theta)*tf.math.log(K.sum(tf.math.exp(-theta*J))/batch_sz)
    else:
        if theta == 0:
            J_out=J
        else:
            # if theta != 0 then we we want samples of e^{-\theta (...)} to estimate E[e^{-\theta (...)} | F_t]
            J_out=tf.math.exp(-theta*J)
    
    return J_out

def create_plots(n_samples):
    n_t_sim2=n_steps*24

    perc=np.zeros([n_theta,n_t_sim2])
    M_t_mean=np.zeros([n_theta,n_t_sim2])
    PV_mean=np.zeros([n_theta,n_t_sim2])
    
    for ind_th in range(0,n_theta):
        theta=thetas[ind_th]
    
        M_t=np.zeros([n_samples,n_t_sim2])
        BV_t=np.zeros([n_samples,n_t_sim2])
        BP_t=np.zeros([n_samples,n_t_sim2])
        PV_t=np.zeros([n_samples,n_t_sim2])
        J_d=np.zeros([n_samples,n_steps])
    
        # The first day will be the last day we simulated
        BV_t[:,0:24]=BV_opt_save[ind_th][0:n_samples,:]
        BP_t[:,0:24]=BP_opt_save[ind_th][0:n_samples,:]
    
        M_trial=np.zeros([n_samples,37])
        PV_trial=np.zeros([n_samples,36])
        PV_trial[:,0:12]=BV_pre[0:n_samples,:]
        PV_trial[:,12:36]=BV_t[0:n_samples,0:24]*(BP_t[0:n_samples,0:24]<=R_t[0:n_samples,12:36])
        M_trial[:,0]=M_0[0:n_samples]
        F_old=np.zeros(n_samples)
        for t in range(0,36):
            ToProd=PV_trial[:,t]
            if t>=12:
                J_d[:,0]=J_d[:,0]+ToProd*R_t[0:n_samples,t]
            F=ToProd/(eta_0*M_trial[:,t])
            F_pos=np.maximum(np.minimum(M_trial[:,t]-M_min,F), 0)
            M_trial[:,t+1]=np.minimum(M_max,M_trial[:,t]-F_pos+I_t[0:n_samples,t])
        
            ramp=np.absolute(F_old-F_pos)
            F_old=F_pos
        
            if t>=12:
                J_d[:,0]=J_d[:,0]-(ToProd-eta_0*F_pos*M_trial[:,t])*LOP_fine\
                    -ramp*ramp_cost
        if n_steps>1:        
            M_t[:,0:25]=M_trial[:,12:37]
        else:
            M_t=M_trial[:,12:36]
            
        PV_t[:,0:24]=PV_trial[:,12:36]
        day=0
    
        for day in range(1,n_steps):
            # Setup input vector
            p_hour=day*24 # Counting starts at 12:00 of day -1
            x_in=np.zeros([n_samples,n_x])
            x_in[:,0]=M_t[:,p_hour-12]
            x_in[:,1]=R_t[0:n_samples,p_hour+12] # Price at the end of the day
            w_pts=p_hour*k+np.array([0,len_weather_vec])
            x_in[:,2:2+len_weather_vec]=W_t[0:n_samples,w_pts[0]:w_pts[1]]
            x_in[:,2+len_weather_vec:2+len_weather_vec+12]= PV_t[:,p_hour:p_hour+12]
            bids_out = mdl_bids[ind_th][day].predict([x_in,R_t_data[0:n_samples,:], W_t_last[0:n_samples,:], I_t_data[0:n_samples,:]])
        
            BV_t[:,day*24:(day+1)*24]=bids_out[:,n_out-48:n_out-24]
            BP_t[:,day*24:(day+1)*24]=bids_out[:,n_out-24:n_out]
        
            PV_t[:,day*24:(day+1)*24]=BV_t[:,day*24:(day+1)*24]*\
                (BP_t[:,day*24:(day+1)*24]<=R_t[0:n_samples,day*24+12:(day+1)*24+12])
            
            for t in range(day*24,(day+1)*24):
                ToProd=PV_t[:,t]
                J_d[:,day]=J_d[:,day]+ToProd*R_t[0:n_samples,t+12]
                F=ToProd/(eta_0*M_t[:,t])
                F_pos=np.maximum(np.minimum(M_t[:,t]-M_min,F), 0)
                ramp=np.absolute(F_old-F_pos)
                F_old=F_pos
                if t+1<n_t_sim2:
                    M_t[:,t+1]=np.minimum(M_max,M_t[:,t]-F_pos+I_t[0:n_samples,t+12])
                J_d[:,day]=J_d[:,day]-(ToProd-eta_0*F_pos*M_t[:,t])*LOP_fine\
                    -ramp*ramp_cost
        
        plt.plot(M_t[0:100,:].transpose())
        plt.title("Volume in reservoir")
        plt.xlabel("Hour")
        plt.show()

        perc[ind_th,:]=np.percentile(M_t,5,axis=0)
        M_t_mean[ind_th,:]=np.mean(M_t,axis=0)
        PV_mean[ind_th,:]=np.mean(PV_t,axis=0)

    plt.figure()
    for ind_th in range(0,n_theta):
      plt.plot(M_t_mean[ind_th], label=r"$\theta = " + str(thetas[ind_th]) + "$")
    plt.title("Average reservoir level")
    plt.legend(loc="best")
    plt.tight_layout()
    plt.show()

    plt.figure()
    for ind_th in range(0,n_theta):
      plt.plot(perc[ind_th], label=r"$\theta = " + str(thetas[ind_th]) + "$")
    plt.title("Reservoir level percentiles")
    plt.legend(loc="best")
    plt.tight_layout()
    plt.show()
    
    plt.figure()
    for ind_th in range(0,n_theta):
      plt.plot(PV_mean[ind_th], label=r"$\theta = " + str(thetas[ind_th]) + "$")
    plt.title("Average production")
    plt.xlabel("Hour")
    plt.legend(loc="best")
    plt.tight_layout()
    plt.show()

    # Plot value functions
    n_samp=101
    M_t_samp=np.linspace(M_min, M_max, n_samp)
    x_in=np.zeros([n_samp,n_x])
    x_in[:,0]=M_t_samp # M_t
    x_in[:,1]=np.ones(n_samp) # R_t
    
    v_funs=np.zeros([n_theta,n_samp])

    plt.figure()

    for ind_th in range(0,n_theta):
        temp=mdl_E_exp[ind_th][0].predict(x_in)
        theta=thetas[ind_th]
        if theta != 0:
            # if theta != 0 then the rewards are samples of of e^{-\theta (...)}
            temp = (-1./theta) * np.log(temp)
        v_funs[ind_th,:]=np.reshape(temp,[1,-1])
        plt.plot(M_t_samp,v_funs[ind_th], label=r"$\theta = " + str(theta) + "$")
    
    plt.title("Value function at t=0")
    plt.xlabel("Volume in the reservoir")
    plt.legend(loc="best")
    plt.tight_layout()
    plt.show()

    return PV_mean
    
#############################

def makeTrainable(layer):
    layer.trainable = True

    if hasattr(layer, 'layers'):
        for l in layer.layers:
            makeTrainable(l)

# Now to setup the Neural Networks
def makeUntrainable(layer):
    layer.trainable = False

    if hasattr(layer, 'layers'):
        for l in layer.layers:
            makeUntrainable(l)

#############################

# Inputs
len_weather_vec = np.int(delay*k)+1
W_t_last_sim  = Input(shape=(len_weather_vec,)) # part of state next day

# Guess values for bids at start
len_bid_vec = 12
BV_pre = 2. * np.random.rand(num_samples,1) + np.zeros([1,len_bid_vec])

# print("BV_pre:", BV_pre)

n_x=2+len_weather_vec+len_bid_vec
x_input = Input(shape=(n_x,)) # Input to NN: [M_0,R_0,W_hist,B_pre]

len_price_vec = 24
R_t_sim = Input(shape=(len_price_vec,)) # Samples of R_t, 00:00 - 23:00

len_inflow_vec = 36
I_t_sim = Input(shape=(len_inflow_vec,)) # Inflows -12:00 - 23:00

R_t_data=R_t[:,12:36]
n_W_t=np.int((36+delay)/k)+1
W_t_last=W_t[:,n_W_t-(len_weather_vec):n_W_t]

I_t_data=I_t[:,0:36]

#######################################################

len_bv = 24 # length of the volume bid vector
len_bp = 24 # length of the price bid vector

mdl_bids = [[None] * n_steps for i in range(n_theta)];
mdl_E_exp = [[None] * n_steps for i in range(n_theta)]; # Approximates E[ e^{-\theta*(X_t + V_{t+1})} |F_t]

BV_opt_save=[None]*n_theta
BP_opt_save=[None]*n_theta

mean_rewards = np.zeros([n_theta,n_steps])
mean_rewards_pred = np.zeros([n_theta,n_steps])
missmatch = np.zeros([n_theta,n_steps])

tid1=time.perf_counter()

first_run=True
fit_bids=True

for day in range(T,-1,-1):
    print("Working on t="+str(day))
    
    for theta_ind in range(0,n_theta):
        
        fit_bids=True
        
        theta=thetas[theta_ind]
        
        print(f"Working on theta={theta}")
        
        x_data=np.zeros([num_samples,n_x])
        x_data[:,0]=M_0
        x_data[:,1]=R_t[:,11]
        x_data[:,2:2+len_weather_vec]=W_t[:,:len_weather_vec]
        x_data[:,2+len_weather_vec:2+len_weather_vec+len_bid_vec]=BV_pre

        y_in=np.zeros([num_samples,1])
        
        # create the bid neural networks
        
        # BV_out is in [0,2] so we multiply [0,1] output from sigmoid function by 2
        BV_out = tf.keras.Sequential([
            Dense(100, activation='sigmoid'),
            Dense(200, activation='sigmoid'),
            Dense(len_bv, activation='sigmoid')
        ])(x_input) * 2

        # BP_out is in [0,4] so we multiply [0,1] output from sigmoid function by 4
        BP_out = tf.keras.Sequential([
            Dense(100, activation='sigmoid'),
            Dense(200, activation='sigmoid'),
            Dense(len_bp, activation='sigmoid')
        ])(x_input) * 4

        mdl_bids_out=tf.concat([x_input,R_t_sim,W_t_last_sim,I_t_sim,BV_out,BP_out],
                                axis=-1)
      
        mdl_bids[theta_ind][day] = Model(inputs=[x_input,R_t_sim,W_t_last_sim,I_t_sim],
                                         outputs=mdl_bids_out)

        mdl_bids[theta_ind][day].compile(loss=get_revenue,optimizer='adam')

        mdl_bids[theta_ind][day].fit([x_data,R_t_data,W_t_last,I_t_data], y_in, epochs=num_epochs_bids,
                                      batch_size=batch_sz, callbacks=[early_stopping_bids])

        bids_out=mdl_bids[theta_ind][day].predict([x_data,R_t_data,W_t_last,I_t_data])
        
        n_out=bids_out.shape[1]
        BV_opt=bids_out[:,n_out-48:n_out-24]
        BP_opt=bids_out[:,n_out-24:n_out]
        
        if day==0:
            BV_opt_save[theta_ind]=BV_opt
            BP_opt_save[theta_ind]=BP_opt
            
        ######## Train value function approximation network ########
        
        fit_bids=False
            
        y_true=np.concatenate((x_data,R_t_data,W_t_last,I_t_data,BV_opt,BP_opt),
                              axis=-1)
        
        rewards=get_revenue([],y_true)
        rewards=tf.reshape(rewards,[-1,1]) # np.reshape
            
        if theta == 0:
            mean_rewards[theta_ind,day] = np.mean(rewards)
        else:
            # if theta != 0 then the rewards are samples of of e^{-\theta (...)}
            mean_rewards[theta_ind,day] = -(1./theta)*tf.math.log(tf.reduce_mean(rewards)).numpy() # np.log, np.mean
            
        print(f"Day {day} finished with average reward {np.round(mean_rewards[theta_ind, day],2)} for theta={theta} \n")
        
        v_input = Input(shape=(n_x,))
            
        if theta == 0:
            # we expect the output to be non-negative: [0,\infty]
            # therefore elu and relu are good candidates for output layer
            vfun = tf.keras.Sequential([
                Dense(40, activation='sigmoid'),
                Dense(40, activation='sigmoid'),
                Dense(1, activation='elu')
            ])(v_input)
        else:
            # if theta != 0 then the rewards are samples of of e^{-\theta (...)}
            # which we expect to be between [0,1]
            # therefore sigmoid is a good candidate for output layer
            vfun = tf.keras.Sequential([
                Dense(40, activation='sigmoid'),
                Dense(40, activation='sigmoid'),
                Dense(1, activation='sigmoid')
            ])(v_input)

        trials_nn = Model(inputs=v_input,outputs=vfun) # temp model
        
        it=0
        while it < 10:
            it += 1

            # fit temp model for value function
            trials_nn.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer='adam')

            trials_nn.fit(x_data, rewards, epochs=num_epochs_value, batch_size=batch_sz, 
                          callbacks=[early_stopping_value_func])

            rewards_pred=trials_nn.predict(x_data)
            miss_this=rewards-rewards_pred
           
            if theta == 0:
                mean_rewards_pred[theta_ind,day] = np.mean(rewards_pred)
            else:
                # if theta != 0 then the rewards are samples of of e^{-\theta (...)}
                mean_rewards_pred[theta_ind,day] = -(1./theta)*np.log(np.mean(rewards_pred))

            meanmiss=np.mean(np.absolute(miss_this))
            bias=mean_rewards[theta_ind,day]-mean_rewards_pred[theta_ind,day]
            rel_bias=np.absolute(bias/mean_rewards[theta_ind,day])

            if rel_bias < 0.05:
                # exit loop early if abs relative error below tolerance
                break
            
            print("\n")
        
        mdl_E_exp[theta_ind][day] = trials_nn
        makeUntrainable(mdl_E_exp[theta_ind][day])
        
        missmatch[theta_ind,day]=meanmiss
        print(['Mean expected rewards:', mean_rewards])
        print(['Run finished, missmatch:',missmatch])
        print(['Relative error in value function output:', rel_bias])
        plt.plot(M_0,rewards_pred,"ob")
        plt.show()
        plt.plot(M_0,rewards,"ob")
        plt.show()

    first_run=False

total_tid=np.round((time.perf_counter()-tid1)/60,2)

print(['Training executed in ', total_tid,' minutes'])

print(['Average rewards ', np.round(mean_rewards,2)])

plt.plot(np.mean(BV_opt,axis=0))
plt.show()

# For illustrations we simulate some output trajectories
create_plots(10000)
